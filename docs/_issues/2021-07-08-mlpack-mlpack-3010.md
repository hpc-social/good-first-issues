---
tags: ,c-binding,c-methods,help-wanted,s-keep-open,t-bug-report
title: "Default values of Adaboost program are not good"
html_url: "https://github.com/mlpack/mlpack/issues/3010"
user: NippunSharma
repo: mlpack/mlpack
---

#### Issue description

This issue was encountered when I was testing the python generated wrapper of Adaboost for my GSoC project for compatibility with scikit hyperparameter tuners in Python.

#### Steps to reproduce

I am pasting the code that I used but the changes that I am working on are still not merged into mlpack.
However, I am also pasting the best parameters and results that I got after tuning. To reproduce this, you can
try to run the currently available `adaboost()` program through python with the given parameters and confirm the results.

```python
from skopt import BayesSearchCV
from mlpack import Adaboost
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import numpy as np

X, y = load_digits(n_class=10, return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)

# training a model with default parameters and finding out the score.
modelNotTuned = Adaboost()
modelNotTuned.fit(X_train, y_train)
preds = modelNotTuned.predict(X_test)
f1_score(y_test, preds, average="weighted")
# output:
# 0.05856060206432102 (very bad)

# Now, tuning the model.
opt = BayesSearchCV(
    Adaboost(),
    {
        'iterations': (100, 500),
        'weak_learner': ['perceptron', 'decision_stump']
    },
    n_iter=10,
    cv=3,
    scoring="f1_weighted",
    random_state=1,
)

opt.fit(X_train, y_train)

print("val. score: %s" % opt.best_score_)
print("test score: %s" % opt.score(X_test, y_test))
# output: 
# val. score: 0.9554649920291841
# test score: 0.9558136593062766 (much better)

opt.best_params_
# output:
# OrderedDict([('iterations', 226), ('weak_learner', 'perceptron')])
```
#### Expected behavior
While we do expect hyperparameter tuning to improve the results, default parameters should
at least be able to provide some decent enough results.

#### Actual behavior
The default parameters provided very bad results.

#### Remarks
Some research should be done on finding out a set of parameters that can provide a decent score
in the majority of tasks. For reference, scikit's `AdaBoostClassifier()` gives a weighted f1 score of about
0.19 (on the same dataset shown in the example), so we should aim to achieve at least that much.

